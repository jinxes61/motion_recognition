{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/data_mining/.local/lib/python3.6/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (9531, 200, 60)\n",
      "Y_train shape: (9531,)\n",
      "X_test shape: (2383, 200, 60)\n",
      "Y_test shape: (2383,)\n"
     ]
    }
   ],
   "source": [
    "import h5py\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.nn.utils import weight_norm\n",
    "%matplotlib inline\n",
    "\n",
    "def convert_to_one_hot(Y, C):\n",
    "    Y = np.eye(C)[Y.reshape(-1)].T\n",
    "    return Y\n",
    "\n",
    "####### 修改输入文件\n",
    "file = h5py.File('DB2/wavelet_200_60.h5','r')\n",
    "imageData   = file['imageData'][:]\n",
    "imageLabel  = file['imageLabel'][:]  \n",
    "file.close()\n",
    "\n",
    "# 随机打乱数据和标签\n",
    "N = imageData.shape[0]\n",
    "index = np.random.permutation(N)\n",
    "data  = imageData[index,:,:]\n",
    "label = imageLabel[index]\n",
    "\n",
    "# 对数据升维,标签one-hot\n",
    "# data  = np.expand_dims(data, axis=1)\n",
    "label = convert_to_one_hot(label,49).T\n",
    "label = np.argmax(label, axis=1)  \n",
    "\n",
    "# 划分数据集\n",
    "N = data.shape[0]\n",
    "num_train = round(N*0.8)\n",
    "X_train = data[0:num_train,:,:]\n",
    "Y_train = label[0:num_train]\n",
    "X_test  = data[num_train:N,:,:]\n",
    "Y_test  = label[num_train:N]\n",
    "\n",
    "print (\"X_train shape: \" + str(X_train.shape))\n",
    "print (\"Y_train shape: \" + str(Y_train.shape))\n",
    "print (\"X_test shape: \" + str(X_test.shape))\n",
    "print (\"Y_test shape: \" + str(Y_test.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class dataset_prediction(Dataset):\n",
    "    def __init__(self, data_features, data_target):\n",
    "        self.len = len(data_features)\n",
    "        self.features = torch.from_numpy(data_features)\n",
    "        self.target = torch.from_numpy(data_target)\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        return self.features[index], self.target[index]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "\n",
    "train_set = dataset_prediction(data_features=X_train, data_target=Y_train)\n",
    "test_set = dataset_prediction(data_features=X_test, data_target=Y_test)\n",
    "\n",
    "train_loader = DataLoader(dataset=train_set,\n",
    "                            batch_size=64,\n",
    "                            shuffle=True,\n",
    "                            drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Chomp1d(nn.Module):\n",
    "    def __init__(self, chomp_size):\n",
    "        super(Chomp1d, self).__init__()\n",
    "        self.chomp_size = chomp_size\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        其实这就是一个裁剪的模块，裁剪多出来的padding\n",
    "        \"\"\"\n",
    "        return x[:, :, :-self.chomp_size].contiguous()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TemporalBlock(nn.Module):\n",
    "    def __init__(self, n_inputs, n_outputs, kernel_size, stride, dilation, padding, dropout=0.2):\n",
    "        \"\"\"\n",
    "        相当于一个Residual block\n",
    "\n",
    "        :param n_inputs: int, 输入通道数\n",
    "        :param n_outputs: int, 输出通道数\n",
    "        :param kernel_size: int, 卷积核尺寸\n",
    "        :param stride: int, 步长，一般为1\n",
    "        :param dilation: int, 膨胀系数\n",
    "        :param padding: int, 填充系数\n",
    "        :param dropout: float, dropout比率\n",
    "        \"\"\"\n",
    "        super(TemporalBlock, self).__init__()\n",
    "        self.conv1 = weight_norm(nn.Conv1d(n_inputs, n_outputs, kernel_size,\n",
    "                                           stride=stride, padding=padding, dilation=dilation))\n",
    "        # 经过conv1，输出的size其实是(Batch, input_channel, seq_len + padding)\n",
    "        self.chomp1 = Chomp1d(padding)  # 裁剪掉多出来的padding部分，维持输出时间步为seq_len\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "\n",
    "        self.conv2 = weight_norm(nn.Conv1d(n_outputs, n_outputs, kernel_size,\n",
    "                                           stride=stride, padding=padding, dilation=dilation))\n",
    "        self.chomp2 = Chomp1d(padding)  #  裁剪掉多出来的padding部分，维持输出时间步为seq_len\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "\n",
    "        self.net = nn.Sequential(self.conv1, self.chomp1, self.relu1, self.dropout1,\n",
    "                                 self.conv2, self.chomp2, self.relu2, self.dropout2)\n",
    "        self.downsample = nn.Conv1d(n_inputs, n_outputs, 1) if n_inputs != n_outputs else None\n",
    "        self.relu = nn.ReLU()\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        \"\"\"\n",
    "        参数初始化\n",
    "\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        self.conv1.weight.data.normal_(0, 0.01)\n",
    "        self.conv2.weight.data.normal_(0, 0.01)\n",
    "        if self.downsample is not None:\n",
    "            self.downsample.weight.data.normal_(0, 0.01)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        :param x: size of (Batch, input_channel, seq_len)\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        out = self.net(x)\n",
    "        res = x if self.downsample is None else self.downsample(x)\n",
    "        return self.relu(out + res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TemporalConvNet(nn.Module):\n",
    "    def __init__(self, num_inputs, num_channels, kernel_size=2, dropout=0.2):\n",
    "        \"\"\"\n",
    "        TCN，目前paper给出的TCN结构很好的支持每个时刻为一个数的情况，即sequence结构，\n",
    "        对于每个时刻为一个向量这种一维结构，勉强可以把向量拆成若干该时刻的输入通道，\n",
    "        对于每个时刻为一个矩阵或更高维图像的情况，就不太好办。\n",
    "\n",
    "        :param num_inputs: int， 输入通道数\n",
    "        :param num_channels: list，每层的hidden_channel数，例如[25,25,25,25]表示有4个隐层，每层hidden_channel数为25\n",
    "        :param kernel_size: int, 卷积核尺寸\n",
    "        :param dropout: float, drop_out比率\n",
    "        \"\"\"\n",
    "        super(TemporalConvNet, self).__init__()\n",
    "        layers = []\n",
    "        num_levels = len(num_channels)\n",
    "        for i in range(num_levels):\n",
    "            dilation_size = 2 ** i   # 膨胀系数：1，2，4，8……\n",
    "            in_channels = num_inputs if i == 0 else num_channels[i-1]  # 确定每一层的输入通道数\n",
    "            out_channels = num_channels[i]  # 确定每一层的输出通道数\n",
    "            layers += [TemporalBlock(in_channels, out_channels, kernel_size, stride=1, dilation=dilation_size,\n",
    "                                     padding=(kernel_size-1) * dilation_size, dropout=dropout)]\n",
    "\n",
    "        self.network = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.tensor(x)\n",
    "        x = x.to(torch.float32)\n",
    "        x = x.cuda()\n",
    "        \"\"\"\n",
    "        输入x的结构不同于RNN，一般RNN的size为(Batch, seq_len, channels)或者(seq_len, Batch, channels)，\n",
    "        这里把seq_len放在channels后面，把所有时间步的数据拼起来，当做Conv1d的输入尺寸，实现卷积跨时间步的操作，\n",
    "        很巧妙的设计。\n",
    "        \n",
    "        :param x: size of (Batch, input_channel, seq_len)\n",
    "        :return: size of (Batch, output_channel, seq_len)\n",
    "        \"\"\"\n",
    "        return self.network(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TemporalConvNet(\n",
      "  (network): Sequential(\n",
      "    (0): TemporalBlock(\n",
      "      (conv1): Conv1d(200, 25, kernel_size=(2,), stride=(1,), padding=(1,))\n",
      "      (chomp1): Chomp1d()\n",
      "      (relu1): ReLU()\n",
      "      (dropout1): Dropout(p=0.2, inplace=False)\n",
      "      (conv2): Conv1d(25, 25, kernel_size=(2,), stride=(1,), padding=(1,))\n",
      "      (chomp2): Chomp1d()\n",
      "      (relu2): ReLU()\n",
      "      (dropout2): Dropout(p=0.2, inplace=False)\n",
      "      (net): Sequential(\n",
      "        (0): Conv1d(200, 25, kernel_size=(2,), stride=(1,), padding=(1,))\n",
      "        (1): Chomp1d()\n",
      "        (2): ReLU()\n",
      "        (3): Dropout(p=0.2, inplace=False)\n",
      "        (4): Conv1d(25, 25, kernel_size=(2,), stride=(1,), padding=(1,))\n",
      "        (5): Chomp1d()\n",
      "        (6): ReLU()\n",
      "        (7): Dropout(p=0.2, inplace=False)\n",
      "      )\n",
      "      (downsample): Conv1d(200, 25, kernel_size=(1,), stride=(1,))\n",
      "      (relu): ReLU()\n",
      "    )\n",
      "    (1): TemporalBlock(\n",
      "      (conv1): Conv1d(25, 25, kernel_size=(2,), stride=(1,), padding=(2,), dilation=(2,))\n",
      "      (chomp1): Chomp1d()\n",
      "      (relu1): ReLU()\n",
      "      (dropout1): Dropout(p=0.2, inplace=False)\n",
      "      (conv2): Conv1d(25, 25, kernel_size=(2,), stride=(1,), padding=(2,), dilation=(2,))\n",
      "      (chomp2): Chomp1d()\n",
      "      (relu2): ReLU()\n",
      "      (dropout2): Dropout(p=0.2, inplace=False)\n",
      "      (net): Sequential(\n",
      "        (0): Conv1d(25, 25, kernel_size=(2,), stride=(1,), padding=(2,), dilation=(2,))\n",
      "        (1): Chomp1d()\n",
      "        (2): ReLU()\n",
      "        (3): Dropout(p=0.2, inplace=False)\n",
      "        (4): Conv1d(25, 25, kernel_size=(2,), stride=(1,), padding=(2,), dilation=(2,))\n",
      "        (5): Chomp1d()\n",
      "        (6): ReLU()\n",
      "        (7): Dropout(p=0.2, inplace=False)\n",
      "      )\n",
      "      (relu): ReLU()\n",
      "    )\n",
      "    (2): TemporalBlock(\n",
      "      (conv1): Conv1d(25, 25, kernel_size=(2,), stride=(1,), padding=(4,), dilation=(4,))\n",
      "      (chomp1): Chomp1d()\n",
      "      (relu1): ReLU()\n",
      "      (dropout1): Dropout(p=0.2, inplace=False)\n",
      "      (conv2): Conv1d(25, 25, kernel_size=(2,), stride=(1,), padding=(4,), dilation=(4,))\n",
      "      (chomp2): Chomp1d()\n",
      "      (relu2): ReLU()\n",
      "      (dropout2): Dropout(p=0.2, inplace=False)\n",
      "      (net): Sequential(\n",
      "        (0): Conv1d(25, 25, kernel_size=(2,), stride=(1,), padding=(4,), dilation=(4,))\n",
      "        (1): Chomp1d()\n",
      "        (2): ReLU()\n",
      "        (3): Dropout(p=0.2, inplace=False)\n",
      "        (4): Conv1d(25, 25, kernel_size=(2,), stride=(1,), padding=(4,), dilation=(4,))\n",
      "        (5): Chomp1d()\n",
      "        (6): ReLU()\n",
      "        (7): Dropout(p=0.2, inplace=False)\n",
      "      )\n",
      "      (relu): ReLU()\n",
      "    )\n",
      "    (3): TemporalBlock(\n",
      "      (conv1): Conv1d(25, 25, kernel_size=(2,), stride=(1,), padding=(8,), dilation=(8,))\n",
      "      (chomp1): Chomp1d()\n",
      "      (relu1): ReLU()\n",
      "      (dropout1): Dropout(p=0.2, inplace=False)\n",
      "      (conv2): Conv1d(25, 25, kernel_size=(2,), stride=(1,), padding=(8,), dilation=(8,))\n",
      "      (chomp2): Chomp1d()\n",
      "      (relu2): ReLU()\n",
      "      (dropout2): Dropout(p=0.2, inplace=False)\n",
      "      (net): Sequential(\n",
      "        (0): Conv1d(25, 25, kernel_size=(2,), stride=(1,), padding=(8,), dilation=(8,))\n",
      "        (1): Chomp1d()\n",
      "        (2): ReLU()\n",
      "        (3): Dropout(p=0.2, inplace=False)\n",
      "        (4): Conv1d(25, 25, kernel_size=(2,), stride=(1,), padding=(8,), dilation=(8,))\n",
      "        (5): Chomp1d()\n",
      "        (6): ReLU()\n",
      "        (7): Dropout(p=0.2, inplace=False)\n",
      "      )\n",
      "      (relu): ReLU()\n",
      "    )\n",
      "  )\n",
      ")\n",
      "进行第0个epoch\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/data_mining/.local/lib/python3.6/site-packages/ipykernel_launcher.py:26: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Expected target size [64, 60], got [64]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-d3c1a8d92bfa>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtcn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_x\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/nn/modules/loss.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m   1150\u001b[0m         return F.cross_entropy(input, target, weight=self.weight,\n\u001b[1;32m   1151\u001b[0m                                \u001b[0mignore_index\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mignore_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduction\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1152\u001b[0;31m                                label_smoothing=self.label_smoothing)\n\u001b[0m\u001b[1;32m   1153\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1154\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mcross_entropy\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001b[0m\n\u001b[1;32m   2844\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0msize_average\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mreduce\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2845\u001b[0m         \u001b[0mreduction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_get_string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize_average\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2846\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcross_entropy_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_enum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel_smoothing\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2847\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2848\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Expected target size [64, 60], got [64]"
     ]
    }
   ],
   "source": [
    "num_inputs = 200\n",
    "num_channels = [25, 25, 25, 25]\n",
    "\n",
    "# tcn 实例化\n",
    "tcn = TemporalConvNet(num_inputs=num_inputs, num_channels=num_channels)\n",
    "print(tcn)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    tcn = tcn.cuda()\n",
    "\n",
    "epoches = 100\n",
    "learning_rate = 0.001\n",
    "\n",
    "# 定义优化器和损失函数\n",
    "optimizer = torch.optim.Adam(tcn.parameters(), lr=learning_rate)\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "if torch.cuda.is_available():\n",
    "    loss_function = loss_function.cuda()\n",
    "\n",
    "# 开始训练\n",
    "for epoch in range(epoches):\n",
    "    print(\"进行第{}个epoch\".format(epoch))\n",
    "    for step, (batch_x, batch_y) in enumerate(train_loader):\n",
    "        batch_x = batch_x.cuda()\n",
    "        batch_y = batch_y.cuda()\n",
    "        \n",
    "        output = tcn(batch_x)\n",
    "\n",
    "        loss = loss_function(output, batch_y)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        # 为了实时显示准确率\n",
    "        if step % 64 == 0:\n",
    "            test_output = tcn(X_test).cpu()\n",
    "            pred_y = torch.max(test_output, 1)[1].data.numpy()\n",
    "            accuracy = float(np.sum(pred_y == Y_test)) / float(Y_test.shape[0])\n",
    "            print('Epoch: ', epoch, '| train loss: %.4f' % loss.cpu().data.numpy(), '| test accuracy: %.2f' % accuracy)\n",
    "\n",
    "\n",
    "test_output = tcn(X_test[:10]).cpu()\n",
    "pred_y = torch.max(test_output, 1)[1].data.numpy().squeeze()\n",
    "print(pred_y)\n",
    "print(X_test[:10])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
